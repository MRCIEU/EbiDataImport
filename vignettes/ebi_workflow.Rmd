---
title: EBI upload pipeline
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{EBI upload pipeline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(igdimport)
```

## Complete workflow for one dataset

```{r, eval=FALSE}

# library(devtools)
# load_all()
# ieugwasr::select_api("dev2")

# Authorise
ieugwasr::get_access_token()

# This is a small file, good for testing
ebi_id <- "GCST005522"
x <- EbiDataset$new(
	ebi_id = ebi_id, 
	ftp_path=get_ftp_path(ebi_id),
	igd_id = "ebi-a-EBITEST",
	wd = paste0("uploads/", ebi_id)
)
x$pipeline()
```

Now delete:

```{r, eval=FALSE}
x$api_gwasdata_delete()
```


## Workflow for all datasets

To get a list of datasets that need to be imported:

```{r, eval=FALSE}
ignore_list <- scan(system.file(package="EbiDataImport", "extdata/ebi_ignore_list.txt"), character())
newdats <- determine_new_datasets(blacklist=ignore_list)
newdats <- being_processed(newdats) %>% subset(., need)
```



```{r, eval=FALSE}
ignore <- list()
for(i in 2:nrow(newdats))
{
	message(newdats$ebi_id[i])
	x <- EbiDataset$new(
		ebi_id = newdats$ebi_id[i], 
		ftp_path = newdats$path[i],
		wd = paste0("uploads/", newdats$ebi_id[i])
	)
	o <- x$pipeline()
	if(! "NULL" %in% class(o))
	{
		ignore[[newdats$ebi_id[i]]] <- o
	}
	rm(x)
}
```

## Lookup build

```{r, eval=FALSE}
x <- EbiDataset$new("GCST000755", ftp_path=get_ftp_path("GCST000755"))
x$download_dataset()
x$format_dataset()
```

## Suhre proteins

```{r, eval=FALSE}
ignore_list <- scan(system.file(package="EbiDataImport", "extdata/ebi_ignore_list.txt"), character())
newdats <- determine_new_datasets(blacklist=ignore_list, exclude = FALSE)
suhre <- subset(newdats, grepl("Suhr", path))
suhre <- separate(suhre, path, sep="/", into=c("p1","p2","p3"), remove=F)
temp <- do.call(rbind, strsplit(suhre$p3, split="_"))
suhre$igd_id <- paste0("prot-c-", temp[,1], "_", temp[,2])
suhre$igd_id

traitinfo <- data.table::fread("http://metabolomics.helmholtz-muenchen.de/pgwas/download/probeanno.tsv", sep="\t")
traitinfo$igd_id <- paste0("prot-c-", traitinfo$seqid)

suhre <- merge(suhre, traitinfo, by="igd_id")
save(suhre, file="suhre.rdata")

load("suhre.rdata")
for(i in 1:nrow(suhre))
{
	x <- EbiDataset$new(
		ebi_id=suhre$ebi_id[i],
		ftp_path=suhre$path[i],
		igd_id=suhre$igd_id[i],
		traitname=suhre$target[i]
	)
	x$download_dataset()
	x$pipeline()
}
```

Conclusion - they seem to be missing standard errors? Need to do this through the batch processing.

